---
title: "Analyse Traces"
author: "Nick Malleson"
date: "13 July 2016"
output:
  html_document: 
    toc: yes
    pandoc_args: [
      "+RTS", "-K64m",
      "-RTS"
    ]
  pdf_document:
    fig_crop: no
    highlight: kate
    keep_tex: yes
    latex_engine: xelatex
    number_sections: yes
fontsize: 10pt
---

An extension to the [map_traces.Rmd](./map_traces.Rmd) program. This has the facility to map traces, but focusses on analysing the difference between matched paths and their equivalent shortest path.

The three different kinds of routes that the script reads are:

 - original traces (the original GPX data)
 - matched routes (the original data matched to OSM routes)
 - shortest paths (the shortest paths the could be used to do the same route)

```{r initialise, echo=FALSE, message=FALSE, warning=FALSE}
library(knitr)
knitr::opts_knit$set(root.dir = "/Users/nick/mapping/projects/runkeeper/mitmount/runkeeper/mapmatching-traces/")
setwd("/Users/nick/mapping/projects/runkeeper/mitmount/runkeeper/mapmatching-traces/")
#setwd('~/research_not_syncd/git_projects/surf/projects/BreezeRoutes/traces')

# Paths to the original files, shortest paths, and matched paths
path.org =      "./gpx/"
path.matched =  "./gpx-matched/"
path.shortest = "./gpx-shortest/"

library(GISTools)
#library(rgeos)    # For things like gIntersects
library(rgdal)     # For reading shapefiles
#library(raster)    # For creating regular grids or converting from SpatialPixelsDataFrame to raster
#library(plyr)     # For counting and aggregating
#library(tmap)     # For thematic maps
#library(classInt) # Jenks natural breaks
#library(png)      # For loading pngs after they have been written
#library(grid)     # so that they can be embedded in knitted documents
#library(spdep)    # For doing the spatial regression, contiguity matrices, etc.
#library(GWmodel)  # For geographically weighted regression (GWR)
#library(MASS)     # For stepwise regression (stepAIC())
#library(pander)   # For printing tables nicely
#library(MVN)      # For testing for multivariate normality (MVN)
#library(stats)     # For a-spatial aggregatiion (aggregate)
#library(ggplot2)   # For density scatter plot
#library(hexbin)    # For hexagonal density scatter plots in ggplot
#library(gridExtra) # To put two graphs next to each other in ggplot
#library(plotKML)   # For reading GPX files
#library(OpenStreetMap) # For plotting OSM basemaps
library(parallel) # For ruonning things in parallel (e.g. mclapply())
library(plotKML)   # For reading GPX files
library(data.table) # For adding up distances per user (data.table())
```


# Function to map a trace

It is sometimes useful to visualise a particualr trace. This has come from [map_traces.Rmd](./map_traces.Rmd) - see that file for proper documentation.

```{r map.trace.function }

expanded.bb <- function(bounds) {
  
  bb <- bbox(bounds)
  # Make the bounding box x % larger to make sure none of the map is clipped
  width = bb[1,2] - bb[1,1]
  height = bb[2,2] - bb[2,1]
  x <- 0.10 # 20 % larger in total
  bb[1,1] = bb[1,1] - (width*x)
  bb[1,2] = bb[1,2] + (width*x)
  bb[2,1] = bb[2,1] - (height*x)
  bb[2,2] = bb[2,2] + (height*x)
  return(bb)
}

# A convenience for plotting a trace given a filename relative to cwd
map.file <- function(filename) {
  print(paste("Mapping GPX file",filename))
  # Create SpatialLinesDataFrames for the track
  track <- readOGR(dsn=filename, layer="tracks", verbose=FALSE )  
  # Project to WGS 84 / Pseudo Mercator (epsg:3857) for OSM
  track.merc <- spTransform(track, CRS("+init=epsg:3857"))
  
  bb <- expanded.bb(track@bbox)

  # Get an OSM basemap (coordinates are upper-left and lower-right)
  basemap <- openmap(
    upperLeft = c(bb[2,2],bb[1,1]), # Upper-left (lat,lon) (y,x)
    lowerRight = c(bb[2,1], bb[1,2]), # Lower-right
    type='osm', zoom=17)
  
  par(mfrow=c(1,1))
  plot(basemap)
  plot(track.merc, col='blue', lwd=3)
  title(paste("Track for\n",f), cex=0.5 )
}

# A convenience to call the plotting function defined below (just pass in the list index for the routes)
map.index <- function(N, osm=TRUE) {
  map.trace(orig[[N]], matched[[N]], shortest[[N]], osm=osm)
}

map.trace <- function(orig, matched, short, osm=TRUE) {
  # Get a bounding box for the whole dataset to make sure plots are big enough
  # Need to do this before projecting otherwise openmap() doesn't work (not sure why)
  bb <- expanded.bb(orig@bbox)
  
  # Project to WGS 84 / Pseudo Mercator (epsg:3857) for OSM
  matched.merc <- spTransform(matched, CRS("+init=epsg:3857"))
  orig.merc    <-   spTransform(orig,    CRS("+init=epsg:3857"))
  short.merc <- spTransform(short,    CRS("+init=epsg:3857"))
  
  # Get an OSM basemap (coordinates are upper-left and lower-right)
  if (osm) {
    basemap <- openmap(
      upperLeft = c(bb[2,2],bb[1,1]), # Upper-left (lat,lon) (y,x)
      lowerRight = c(bb[2,1], bb[1,2]), # Lower-right
      type='osm', zoom=17)
  }
  
  par(mfrow=c(1,1))
  if (osm) {
    plot(basemap)
  }
  else {
    plot(orig.merc, col='blue', lwd=3)
  }
  title(paste("Routes for\n",f), cex=0.5 )
  plot(orig.merc, col='blue', lwd=3, add=T)
  plot(matched.merc, lwd=3, col='black', add=T)
  plot(short.merc, col='green', lwd=3, add=T)
  
}

# Get the USER ID from an original gpx file
read.userid <- function(filename) {
  text <- tryCatch( 
    { readLines(filename) },
    error=function(cond) {
      message(paste("Could not read file for username: ", filename, '. Message is:', cond))
      return(-1)
    }
  ) # tryCatch
  if (text==-1) {
    return(-1)
  }
  result <- grep('userId=[A-z0-9]+', text, value=TRUE) # Find the bit with the User ID
  if (length(result)==0) {
    warning(paste("No match for user im file",filename))
    return (-1)
  }
  # Match looks ok, return the userid part.
  uid <- substring(result,8)
  #print(paste("Got UID: ",uid))
  return(uid)
}
  
```

# Read the Data

Read all of the traces and store in memory. Only read 'matched' traces because those will definitely have an original trace and a shortest path. Not all original traces have been matched by the MapMatcher program.


```{r readTraces}

#TRACES_FILE = "./traces.RData"
#if (!file.exists(TRACES_FILE)) {
  # Vectors that will contain all of the paths and associated information.
  orig <- list() 
  matched <- list()
  shortest <- list()
  orig.ma <- list()  # Projected versions
  matched.ma <- list()
  shortest.ma <- list()
  userid <- list()

  # Read files in the 'matched' directory. We're only interested in mapping routes that have been matched 
  # (and if they've ben matched then they probably also have a companion shortest path).
  file.names <- dir(path.matched, pattern =".gpx")
  
  for(i in 1:100){
  #for(i in seq(length(file.names))){
    start.time <- proc.time()[['elapsed']]
    f <- substr(file.names[i], 1, nchar(file.names[i])-12) # The name of the file without the '-matched.gpx' extension.
    f.orig <-    paste(path.org,      f, ".gpx", sep="")
    f.matched <- paste(path.matched,  f, "-matched.gpx", sep="") # The matched file (f531b5395-matched.gpx)
    f.shortest<- paste(path.shortest, f, "-shortest.gpx", sep="") # The shortest path (f531b5395-shortest.gpx)
    print(paste("Reading file (",i,")",f))
    
    # Create SpatialLinesDataFrames for each track, project them to good projection for MA (Albers) add them to the lists
    # (https://www.arcgis.com/home/item.html?id=d075ba0b6b5e4d71b596e882493f7789)
    # Read the three required files simultaneously in parallel
    read.data <- mclapply(
      list("orig"=f.orig, "matched"=f.matched, "shortest"=f.shortest), 
      FUN=function(x){
        # Read the GPX and convert the $tracks to a dataframe. All wrapped in a try-catch in case file can't be read
        gpx <- as.data.frame(
          tryCatch(
            {
              readGPX(x, metadata = TRUE, bounds = TRUE,waypoints = FALSE, tracks = TRUE, routes = FALSE)$tracks
            },
            error=function(cond) {
              message(paste("Could not read file: ", x, '. Message is:', cond))
              return(NULL)
            },
            warning=function(cond) {
              message(paste("Could not read file: ", x, '. Message is:', cond))
              return(NULL)
            }
          ) #trycatch
        ) #as.data.frame
        # Work out where x,y coords are
        if ('GraphHopper.lon' %in% colnames(gpx) ) {
          xcor <- gpx$GraphHopper.lon
          ycor <- gpx$GraphHopper.lat
        } else {
          xcor <- gpx$lon
          ycor <- gpx$lat
        }
        
        lines <- SpatialLines(
          list(Lines(lapply(list(cbind(xcor, ycor)), Line), ID="a")),
          proj4string = CRS("+init=epsg:4326"))
        return(lines)
      }
    )
    orig[[i]]    =  read.data[['orig']]
    matched[[i]] =  read.data[['matched']]
    shortest[[i]] = read.data[['shortest']]
    
    transformed <- mclapply(
      list("orig.ma"=orig[[i]], "matched.ma"=matched[[i]], "shortest.ma"=shortest[[i]]),
      FUN = function(x) { return(spTransform(x, CRS("+init=epsg:5070")) ) }
    )
    orig.ma[[i]] =    transformed[['orig.ma']]
    matched.ma[[i]] = transformed[['matched.ma']]
    shortest.ma[[i]] =transformed[['shortest.ma']]
   
    # To get the user id we need to re-read the original gpx file and parse it manually. The
    # readGPX function doesn't keep the meta-data, and readOGR is too slow.
    uid <- read.userid(f.orig)
    if (uid == -1) {
      warning(paste("Not able to get the user ID for route (",i,")", f ) )
      uid = "NA"
    }
    userid[[i]] = uid
    
    # OLD WAY OF GETTING UID WHEN SING readOGR
    ## Get the user ID. This needs to be done by parsing the description as readOGR didn't bring in the metadata properly)
    ## Split the string on '\n' (fixed=T means treat this as it is, not as a regex), and take the 6th element
    ##which should be a string like: userId=857ac8242e49159c7abb25bb458a7bc9. Then just get the id from that
    #uid_str <- strsplit(as.character(orig[[i]]@data$desc), '\n', fixed=T)[[1]][6] 
    #uid_split <- strsplit(uid_str, '=')[[1]]
    #uid = uid_split[2] # Second element after splitting on '='
    #if (uid_split[1]!="userId") {
    #  warning(paste("Not able to get the user ID for route (",i,")", f ) )
    #  uid = "NA"
    #}
    #userid[[i]] = uid
    
    print(paste("\t.. finished in",round(proc.time()[['elapsed']]-start.time,digits=2),"secs"))
    
  }# for files
    
  
  if (length(matched) != length(orig) || length(matched) != length(shortest) || length(matched) != length(userid)) {
    warning("For some reason there are different numbers of original, matched, and shortest paths.")
  }
  
# Save the traces
#  save(orig, matched, shortest, orig.ma, matched.ma, shortest.ma, userid, file=TRACES_FILE)

#} else {
#  print(paste("Loading traces from file", TRACES_FILE))
#  load(TRACES_FILE)
#}

```

Read `r length(matched)` matched traces, `r length(orig)` original traces, and `r length(short)` shortest paths.

# Compare path distances

## Actual paths v.s. shortest path

Look at the differences in the lengths between actual (matched) paths and shortest paths

```{r compareLengths1 }

orig.lengths    <-  unlist(mclapply(orig.ma, FUN=gLength))
matched.lengths  <- unlist(mclapply(matched.ma, FUN=gLength))
shortest.lengths <- unlist(mclapply(shortest.ma, FUN=gLength))

par(mfrow=c(1,3))
ylim <- c(0,500)
xlim <- c(0,8000)

hist(orig.lengths,     breaks='Scott', xlim=xlim, ylim=ylim )
hist(matched.lengths,  breaks='Scott', xlim=xlim, ylim=ylim )
hist(shortest.lengths, breaks='Scott', xlim=xlim, ylim=ylim )

```

```{r compareLengths2}
par(mfrow=c(1,1))
boxplot(list("Original"=orig.lengths,"Matched"=matched.lengths,"Shortest"=shortest.lengths),ylim=c(0,5000), main="Path lengths (short y axis)", ylab="Path length")
```

## Hausdorff distances

Look at the Hausdorff distances between the paths. This quantifies how similar the paths are.

```{r hausdorffDistances1} 
# Run using lapply as above. This is just slightly more complicated because the distance function requires two 
# arguments as it compares two lines. mapply does this. The function comes first, followed by the arguments 
haus.orig.matched <- unlist(
  mcmapply(
    FUN=gDistance, # Function to calcuate the distance
    orig.ma, matched.ma, # First two arguments
    MoreArgs=list(byid=FALSE, hausdorff=TRUE) # Some other named argments
  )
)
haus.matched.shortest <- unlist(
  mcmapply(
    FUN=gDistance, # Function to calcuate the distance
    matched.ma, shortest.ma, # First two arguments
    MoreArgs=list(byid=FALSE, hausdorff=TRUE) # Some other named argments
  )
)
par(mfrow=c(1,2))
xlim=c(0,1500)
ylim=c(0,1000)
hist(haus.orig.matched, main="Hausdorff distance\nOriginal v.s. Matched", xlab="Distance", breaks="Scott",xlim=xlim,ylim=ylim)
hist(haus.matched.shortest, main="Hausdorff distance\nMatched v.s. Shortest", xlab="Distance",  breaks="Scott",xlim=xlim,ylim=ylim)

```

```{r hausdorffDistances2} 
par(mfrow=c(1,1))
boxplot(list("Matched<->Original"=haus.orig.matched, "Matched<->Shortest"=haus.matched.shortest), main="Hausdorff Distances (short y axis)",ylim=c(0,1000))

```

# Explore User distances

Look at the range of distances travelled by individual users. Is there some regularity?

First the number of traces per user.

```{r userDistances}
# Improved histogram for integers (https://mikelove.wordpress.com/2011/03/30/r-one-liner-histogram-for-integers/)
int.hist = function(x,ylab="Frequency",...) {
  barplot(table(factor(x,levels=min(x):max(x))),space=0,xaxt="n",ylab=ylab,...);axis(1)
}

# First find the frequency distribution of trips per user

uid.freq = as.data.frame(table(as.factor(unlist(userid))))
int.hist(uid.freq$Freq, main="Traces per user", xlab="Number of traces", ylab="Frequency (number of users)")
```

Now look at the distributions of mean and standard deviations

```{r userDistancesDist}

# A list of users and their total distances. Uses data.table. https://stackoverflow.com/questions/11782030/sum-by-distinct-column-value-in-r
uid.total.dists <- data.table(
  "userid"=unlist(userid), 
  "orig.lengths"=unlist(orig.lengths), 
  "matched.lengths"=unlist(matched.lengths), 
  "shortest.lengths"=unlist(shortest.lengths), 
  key="userid")
# Now sum that data table
uid.total.dists <- uid.total.dists[,list(sum(orig.lengths), sum(matched.lengths), sum(shortest.lengths), (.N) ), by=userid]
names(uid.total.dists) <- c("userid", "orig", "matched", "shortest", "N")

boxplot(list(
  "Orig"=uid.total.dists$orig/uid.total.dists$N, 
  "Shortest"=uid.total.dists$matched/uid.total.dists$N,
  "Matched"=uid.total.dists$matched/uid.total.dists$N),
  main="Average distances per user")

```

# Quantify Symmetry

## Individual-level symmetry

Identify return journeys - two paths from a particular user where the original origins and final destiantion are the same (or close anyway). 
 
## Aggregate symmetry

Replicate the work in Phithakkitnukoon and Ratti (2011):

Phithakkitnukoon, Santi, and Carlo Ratti (2011). Inferring Asymmetry of Inhabitant Flow Using Call Detail Records. _Journal of Advances in Information Technology_ 2(4). http://ojs.academypublisher.com/index.php/jait/article/view/jait0204239249.

XXXX