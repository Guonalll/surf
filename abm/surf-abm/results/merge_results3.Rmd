---
title: "Analysing surf results across multiple runs."
author: "Nick Malleson"
date: '`r format(Sys.time(), "%d %B, %Y (%H:%M)")`'
output:
  html_document: 
    toc: yes
    pandoc_args: [
      "+RTS", "-K64m",
      "-RTS"
    ]
  pdf_document:
    fig_crop: no
    highlight: kate
    keep_tex: yes
    latex_engine: xelatex
    number_sections: yes
fontsize: 10pt
---

Changelog
 - v2: This second version of the file was prepared after the first round of paper revisions and can be used to compare the two different scenarios by changing the `SCENARIO` parameter. It also dropped the last spatial analysis section and introduced scaling for the size of the population in the model and noggin data to make them comparible.
 - v3: Realised that there is no need to read the agents files in. Only need the camera counts. So have taken loads of annoying technical stuff that was needed to handle reading in hundreds of millions of unnecessary lines in agents.csv :-()



Set up the directories. Note: The script needs the `noggin_data.feather` file to be in the working directory. This is generated by the data and scripts in:

`~/mapping/projects/frl/otley/noggin_data`

```{r initialise, echo=FALSE, message=FALSE, warning=FALSE}
library(GISTools)
#library(rgeos)    # For things like gIntersects
#library(rgdal)     # For reading shapefiles
#library(raster)    # For creating regular grids
#library(plyr)     # For counting and aggregating
#library(tmap)     # For thematic maps
library(classInt) # Jenks natural breaks
#library(png)      # For loading pngs after they have been written
#library(grid)     # so that they can be embedded in knitted documents
#library(spdep)    # For doing the spatial regression, contiguity matrices, etc.
#library(GWmodel)  # For geographically weighted regression (GWR)
#library(MASS)     # For stepwise regression (stepAIC())
#library(pander)   # For printing tables nicely
#library(MVN)      # For testing for multivariate normality (MVN)
library(RColorBrewer) # For making nice colour themes
#library(rgl)       # For 3D space-time cube
#library(plot3D)    # For 3D space-time cube
library(dplyr)     # To look up fields in tables (e.g. N rows higher --> lag)
library(parallel)
library(pbapply)  # For progress bar in parallel
no_cores <- detectCores() / 2  # Detect the number of cores that are available and use half (often CPUs simulate 2 threads per core)
Sys.setenv(MC_CORES=no_cores) # Run on n cores
options("mc.cores"=no_cores)
library(lubridate)
library(hexbin)   # For hex bins
library(ggplot2)  # ditto
library(gridExtra) # For arranging two grids side by side
library(feather)  # For reading data prepared by python
library(leaflet) # For doing interactive maps

SCENARIO1 <- "Commuters"
SCENARIO2 <- "CummutersRetired"

# The ID of the agent to analyse (graphs of all agents can be difficult to understand). If -1 then do a few at random
AGENT_ID <- -1
#AGENT_ID <- 1

# When running this script directly in R(Studio) we need to specify the current working directory
CURRENT_DIRECTORY <- '/Users/nick/research_not_syncd/git_projects/surf/abm/surf-abm/results'

#CURRENT_DIRECTORY <- "C:/Code/surf/abm/surf-abm/results"
setwd(CURRENT_DIRECTORY) # This is only required when running directly in R(Studio)

# Projections. Useful for mapping
BNG <- CRS('+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +datum=OSGB36 +units=m +no_defs')
WGS <- CRS("+proj=longlat +ellps=WGS84 +datum=WGS84")

# Find the directories of all the model runs
scenarios1 <- sort(list.files(paste0("./out/",SCENARIO1))) # The IDs of each run
scenarios.dirs1 <- vapply(X = scenarios1, FUN = function(x) {paste(paste0("./out/",SCENARIO1), "/", x, sep="") }, FUN.VALUE = character(1))
scenarios2 <- sort(list.files(paste0("./out/",SCENARIO2))) # The IDs of each run
scenarios.dirs2 <- vapply(X = scenarios2, FUN = function(x) {paste(paste0("./out/",SCENARIO2), "/", x, sep="") }, FUN.VALUE = character(1))

rm(scenarios1, scenarios2)

# If I have saved the results before, then load them. Calculating dates takes an unusually long amount of time
# After loading skip to 'choose.day' cell
#load('merge_results.RData')
```


# Read and prepare the data

Read the data on the agent locations and activities as well as the camera counts and put into big tables


```{r readData1}

# TODO - include a unique id (the scenario) for each model

# Read the first result to make the tables. The add append the remaining results
cameras1 <- read.csv(paste0(scenarios.dirs1[1],'/camera-counts.csv'))
cameras2 <- read.csv(paste0(scenarios.dirs2[1],'/camera-counts.csv'))

stopifnot(identical(nrow(cameras1),nrow(cameras2)))
stopifnot(identical(length(scenarios.dirs1),length(scenarios.dirs2)))

# Now append the remaining results
for (i in 2:length(scenarios.dirs1)) {
  # Scenario 1
  cameras.temp <- read.csv(paste0(scenarios.dirs1[i],'/camera-counts.csv'))
  cameras1 <- rbind(cameras1, cameras.temp)
  #print(paste("Read file",paste0(scenarios.dirs1[i],'/camera-counts.csv')))
  
  # Scenario 2
  cameras.temp <- read.csv(paste0(scenarios.dirs2[i],'/camera-counts.csv'))
  cameras2 <- rbind(cameras2, cameras.temp)
  #print(paste("Read file",paste0(scenarios.dirs2[i],'/camera-counts.csv')))
  
}
rm(cameras.temp)

# Make proper date columns

make_date_columns <- function(cameras) {
  # Camera dates need to have date plus number of hours as these are stored in different columns (don't bother with parallel)
  cameras$date <- as.POSIXct(as.Date(cameras$Date)) + ( 3600 * cameras$Hour )
  # Cameras also need some extra date columns to make them easier to work with
  cameras$Day <-     as.POSIXct(round(cameras$date, units="days" ))
  cameras$DayNum <-  yday(cameras$date) # Number of day in yaer
  #hour = as.POSIXct(round(modelDf$datetime, units="hours"))
  cameras$Week <-    week(cameras$date) # (from lubridate library)
  cameras$Month <-   month(cameras$date)
  cameras$Weekday <- weekdays(cameras$date)
  # Make the camera ID a string so that ggplot2 treats it as cetegorical
  cameras$Camera <- paste0("",cameras$Camera)
  return(cameras)
}
cameras1 <- make_date_columns(cameras1)
cameras2 <- make_date_columns(cameras2)

# Only analyse the second week to give the model time to reach equilibrium
cameras1 <- cameras1[which(cameras1$DayNum>=8 & cameras1$DayNum<=14),]
cameras2 <- cameras2[which(cameras2$DayNum>=8 & cameras2$DayNum<=14),]

# Read the real camera counts. These data were prepared by scripts and data in: ~/mapping/projects/frl/otley/noggin_data
noggin <- read_feather("noggin_data.feather")
noggin$datetime <- as.POSIXct(noggin$Timestamp)
noggin$Location <- paste0("",noggin$Location) # ID -> String
noggin$Day   <-   as.POSIXct(round(noggin$datetime, units="days" ))
noggin$DayNum <-  yday(noggin$datetime) # Number of day in year
noggin$Hour  <-   as.POSIXct(round(noggin$datetime, units="hours"))
noggin$Week  <-   floor_date(noggin$datetime, "week") # (from lubridate library)
noggin$Month <-   floor_date(noggin$datetime, "month")
noggin$Weekday <- weekdays(noggin$datetime)
noggin$HourOfDay <- as.integer(format(noggin$datetime, "%H")) # (https://stackoverflow.com/questions/10683224/obtain-hour-from-datetime-vector)
# Extract middle of the week
noggin = noggin[which( !(noggin$Location %in% c("16","19")) & noggin$Weekday %in% c("Tuesday","wednesday","Thursday") ),]
noggin[which(noggin$Location=="20"),]$Location = "17" # rename location 20 to location 17 (as per noggin metadata)
#aggnoggin = aggregate(Count~Location + HourOfDay, data=noggin, FUN=sum)
#aggnoggin$RelCount = aggnoggin$Count / sum(aggnoggin$Count)


# Scale using z score
calc.z <- function(x) {
  mean_ <- mean(x)
  sd_ <- sd(x)
  return(sapply(X=1:length(x), FUN=function(i) { (x[i] - mean_) / sd_} ) )
}
cameras1$CountScaled <- calc.z(cameras1$Count)
cameras2$CountScaled <- calc.z(cameras2$Count)
noggin$CountScaled <-   calc.z(noggin$Count)

```

# GRAPH 1: COMMUTING RESULTS (SCENARIO 1)

A graph comparing the footfall in sensors as per the commuting model to the noggin data.

```{r footfall-graph-commuters1, fig.width=9, fig.height=9}

make_footfall_plot <- function(c, aes_, title) {
  ggplot(c, aes_ ) +
  #stat_sum(aes(group = 1))+ # Scatter points scaled by size
  #scale_colour_manual(values=cols)+
  #geom_hex(bins=15) +
  #geom_point(color="black", size=0.5) + 
  geom_smooth(se=TRUE, level=0.99, color="black") + #GAM
  ylab("Mean count (z score)")+
  ggtitle(title)

}
p1 <- make_footfall_plot(cameras1, aes(Hour, CountScaled),    title="Footfall count from all sensors over all model runs - Commuters model")
p3 <- make_footfall_plot(noggin, aes(HourOfDay, CountScaled), title="Footfall count from all sensors - Real data")
g <- grid.arrange(p1, p3, nrow=2)
g

ggsave(filename="footfall-graph-commuters1.pdf",  plot=g, device="pdf",  path="./figs_for_surf_paper/", width=9, height=6)

```


# GRAPH 2: COMMUTING+RETIRED RESULTS (SCENARIO 2)

A graph comparing the footfall in sensors as per the commuting+retired model to the noggin data.


```{r footfall-graph-retired1, fig.width=9, fig.height=5}

# As three separate graphs:
#p1 <- make_footfall_plot(cameras1, aes(Hour, CountScaled),    title="Footfall count from all sensors over all model runs - Commuters model")
#p2 <- make_footfall_plot(cameras2, aes(Hour, CountScaled),    title="Footfall count from all sensors over all model runs - Commuters model")
#p3 <- make_footfall_plot(noggin, aes(HourOfDay, CountScaled), title="Footfall count from all sensors - Real data")
#g <- grid.arrange(p1, p2, p3, nrow=3)
#g

# As one graph

g <- ggplot() +
  geom_smooth(se=TRUE, level=0.99, data=cameras1, aes(Hour, CountScaled ), color="black", linetype="dotted") +
  #geom_text( data=cameras1 %>%   filter(Hour==23,Camera=="14", DayNum==8) %>% View,   aes(x=Hour,y=CountScaled), hjust="left", label="Commuters model") +
  geom_text( aes(x=23.5,y=-0.6), hjust="left", label="Commuters & retired") +
  geom_smooth(se=TRUE, level=0.99, data=cameras2, aes(Hour, CountScaled), color="black", linetype="dashed") +
  geom_text( aes(x=23.5,y=-0.45), hjust="left", label="Commuters") +
  geom_smooth(se=TRUE, level=0.99, data=noggin,   aes(HourOfDay, CountScaled), color="black", linetype="solid") +
  geom_text( aes(x=23.5,y=-0.75), hjust="left", label="Real data") +
  scale_x_continuous(limits=c(0,29))+
  ylab("Mean count (z score)")+
  ggtitle("Footfall count from all sensors")

g

ggsave(filename="footfall-graph-retired1.pdf",  plot=g, device="pdf",  path="./figs_for_surf_paper/", width=9, height=5)

```




# GRAPH 3: COMPARING INDIVIDUAL SENSORS

A graph comparing the results of the commuting+retired model to the noggin data by each sensor.


```{r footfall-individual-sensors, fig.width=9, fig.height=5}
# Consistent colours for each sensor
cols <- brewer.pal(length(unique(cameras2$Camera)),'Set1')

ggplot() +
  #geom_point() + 
  geom_smooth(se=TRUE, level=0.99, data=cameras2, aes(Hour, CountScaled, col=Camera), linetype="dotted" ) + 
  geom_smooth(se=TRUE, level=0.99, data=noggin,   aes(HourOfDay, CountScaled, col=Location)) + 
  guides(colour=guide_legend(title="Sensor")) + # Change the title of the legend
  #scale_colour_manual("Set3")+
  scale_colour_manual(values=cols)+
  ylab("Mean count (z score)")+
  ggtitle( "Comparing simulated and real Footfall for individual sensors" ) 

ggsave("footfall-individual-sensors.pdf",  device="pdf",  path="./figs_for_surf_paper/", width=9, height=5)


```





















# Plot model results 

Plot the total average hourly footfall count and 99% confidence intervals. Note that the confidence intervals are so small that they are not visible.

```{r model.hourly.total, fig.width=9, fig.height=5}
cols <- brewer.pal(length(unique(cameras2$Camera)),'Set1')

c <- cameras1
#c <- c[which(c$Hour>5 & c$Hour<22 & c$DayNum==14),]
p1 <- ggplot(c, aes(Hour, Count, col=Camera)) +
  stat_sum(aes(group = 1))+ # Scatter points scaled by size
  scale_colour_manual(values=cols)+
  #geom_hex(bins=15) +
  #geom_point(color="black", size=0.5) + 
  #geom_smooth(method="loess", se=TRUE, level=0.99, color="green") + 
  geom_smooth(se=TRUE, level=0.99, color="black") + #GAM
  ylab("Number of agents")+
  ggtitle( "Total footfall count from all sensors over all model runs")

c <- cameras2
#c <- c[which(c$Hour>5 & c$Hour<22 & c$DayNum==14),]
p2 <- ggplot(c, aes(Hour, Count, col=Camera)) +
  stat_sum(aes(group = 1))+ # Scatter points scaled by size
  scale_colour_manual(values=cols)+
  geom_smooth(se=TRUE, level=0.99, color="black") + #GAM
  ylab("Number of agents")+
  ggtitle( "Total footfall count from all sensors over all model runs")

grid.arrange(p1, p2, ncol=2)


#ggsave(paste0("model_hourly_total",SCENARIO,".pdf"),  device="pdf",  path="./figs_for_surf_paper/", width=9, height=4)
#ggsave(paste0("model_hourly_total",SCENARIO,".tiff"), path="./figs_for_surf_paper/", width=9, height=4)


c <- cameras1[which(cameras1$DayNum==14),]
aggModelDf = aggregate(Count~Camera + Hour, data=c, FUN=sum)
aggModelDf$RelCount = aggModelDf$Count / sum(aggModelDf$Count)

plot(aggregate(Count~Hour, data=aggModelDf, FUN=mean), 
     main="Footfall per hour of day (model)", ylab=ylab, col="black", type="l"
)

```

Plot the average hourly count per camera, again with confindence intervals.

```{r model.hourly.sensors, fig.width=9, fig.height=5 }
# Colours for the cameras 
cols <- brewer.pal(length(unique(cameras$Camera)),'Set1')
names(cols) <- unique(cameras$Camera)

plot1 <- ggplot(cameras, aes(Hour, Count, colour=Camera)) +
  #geom_point(size=1) + 
  guides(colour=guide_legend(title="Sensor")) + # Change the title of the legend
  geom_smooth(se=TRUE, level=0.99) + 
  scale_colour_manual(values=cols)+
  ylab("Number of agents")+
  ggtitle( "Total footfall count from for each sensor over all model runs")
plot1

# Uncomment below to see loess as well

#plot2 <- ggplot(cameras, aes(Hour, Count, col=Camera)) +
#  geom_point() + 
#  geom_smooth(method="loess", se=TRUE, level=0.99) +
#  ggtitle( "Footfall counts per camera over all\n model runs (smoothing with loess)")

#grid.arrange(plot1, plot2, ncol=2)

ggsave(paste0("model_hourly_sensors",SCENARIO,".pdf"),  device="pdf",  path="./figs_for_surf_paper/", width=9, height=4)
ggsave(paste0("model_hourly_sensors",SCENARIO,".tiff"), device="tiff", path="./figs_for_surf_paper/", width=9, height=4)

```

# Plot Real Data

Compare these to the real data

```{r plot.real.data, fig.width=9, fig.height=9 }

plot1 <- ggplot(noggin, aes(HourOfDay, Count)) +
  #geom_hex(bins=15) +
  #geom_smooth(method="loess", se=TRUE, level=0.99, color="green") + 
  geom_smooth(se=TRUE, level=0.99, color="black") + #GAM
  ylab("Number of people")+
  xlab("Hour")
  #ggtitle( "Average weekday footfall count from all sensors (real data)")

plot2 <- ggplot(noggin, aes(HourOfDay, Count, colour=Location)) +
  #geom_point(size=2) + 
  guides(colour=guide_legend(title="Sensor")) + # Change the title of the legend
  geom_smooth(se=TRUE, level=0.99)+
  scale_colour_manual(values=cols)+
  ylab("Number of people")+
  xlab("Hour")+
  ggtitle( "Average weekday footfall count per sensor (real data)")

g <- grid.arrange(plot1, plot2, nrow=2)

ggsave(paste0("plot_real_data",SCENARIO,".pdf"),  plot=g, device="pdf",  path="./figs_for_surf_paper/", width=9, height=9)
ggsave(paste0("plot_real_data",SCENARIO,".tiff"), plot=g, device="tiff", path="./figs_for_surf_paper/", width=9, height=9)

#ylab="Total Count"
#plot(aggregate(Count~HourOfDay, data=aggnoggin, FUN=sum), 
#     main="Footfall per hour of day (observations middle of week)", ylab=ylab, col="black", type="l"
#)
```


# Compare differences in absolute footfall counts

Make a box plot. (_Not going to use this in the paper_)

```{r compare.absolute.footdall, fig.width=11, fig.height=5}

#x <- cbind(aggregate(formula = Count ~ Hour, data = cameras, FUN=mean),
#  aggregate(formula = Count ~ HourOfDay, data = noggin, FUN=mean))


boxplot( 
  aggregate(formula = Count ~ Hour, data = cameras, FUN=mean, label="asd"),
  aggregate(formula = Count ~ HourOfDay, data = noggin, FUN=mean),
  horizontal = TRUE
)

#ggplot() +  
#  geom_boxplot(data=aggregate(formula = Count ~ Hour, data = cameras, FUN=mean), 
#               aes(Hour, Count, group="Simulated Data")) 

  #geom_boxplot(data=noggin,  aes(HourOfDay, Count, group="Real-World Data"))
  # Plot for all sensors
  #geom_boxplot(data=cameras, aes(Hour, Count, col=Camera)) + 
  #geom_boxplot(data=noggin,  aes(HourOfDay, Count, col=Location))


```


# Plot Differences Between the Model and the Real Data

On two separate plots (_this isn't being used in the paper_)

```{r compare.plots, fig.width=11, fig.height=5 }

plot1 <- ggplot(cameras1[which(cameras1$DayNum==14),], aes(Hour, Count, col=Camera)) +
  #geom_point() + 
  geom_smooth(method="loess", se=TRUE, level=0.99) + 
  scale_colour_manual(values=cols)+
  ggtitle( "Total footfall counts per camera (models) 1")

plot2 <- ggplot(cameras2[which(cameras2$DayNum==14),], aes(Hour, Count, col=Camera)) +
  #geom_point() + 
  geom_smooth(se=TRUE, level=0.99) + 
  scale_colour_manual(values=cols)+
  ggtitle( "Total footfall counts per camera (models) 2")

plot3 <- ggplot(noggin, aes(HourOfDay, Count, col=Location)) +
  #geom_point() + 
  geom_smooth(se=TRUE, level=0.99) + #GAM
  scale_colour_manual(values=cols)+
  ggtitle( "Total footfall counts per camera (real data)")

grid.arrange(plot1, plot2, plot3, ncol=3)

#ylab="Total Count"
#plot(aggregate(Count~HourOfDay, data=aggnoggin, FUN=sum), 
#     main="Footfall per hour of day (observations middle of week)", ylab=ylab, col="black", type="l"
#)
```



On the same plot:

```{r compare.plots2, fig.width=11, fig.height=5}

ggplot() +
  #geom_point() + 
  geom_smooth(se=TRUE, level=0.99, data=cameras, aes(Hour, Count, col=Camera), linetype="dotted" ) + 
  geom_smooth(se=TRUE, level=0.99, data=noggin,  aes(HourOfDay, Count, col=Location)) + 
  guides(colour=guide_legend(title="Sensor")) + # Change the title of the legend
  #scale_colour_manual("Set3")+
  scale_colour_manual(values=cols)+
  ggtitle( "Comparing Model Results and 'Simulated 'Real World' Data ")

ggsave(paste0("compare_plots2",SCENARIO,".pdf"),  device="pdf",  path="./figs_for_surf_paper/", width=9, height=5)
ggsave(paste0("compare_plots2",SCENARIO,".tiff"), device="tiff", path="./figs_for_surf_paper/", width=9, height=5)

```

Just the average:

```{r compare.plots3, fig.width=11, fig.height=5}

ggplot() +
  #geom_point() + 
  geom_smooth(se=TRUE, level=0.99, data=cameras, aes(Hour, Count),      color="black", linetype="dotted" ) + 
  geom_smooth(se=TRUE, level=0.99, data=noggin,  aes(HourOfDay, Count), color="black") + 
  guides(colour=guide_legend(title="Sensor")) + # Change the title of the legend
  #scale_colour_manual("Set3")+
  scale_colour_manual(values=cols)+
  ggtitle( "Comparing Model Results and 'Simulated 'Real World' Data ")

ggsave(paste0("compare_plots3",SCENARIO,".pdf"),  device="pdf",  path="./figs_for_surf_paper/", width=9, height=5)
ggsave(paste0("compare_plots3",SCENARIO,".tiff"), device="tiff", path="./figs_for_surf_paper/", width=9, height=5)

```


# Index to 100 and Calculate Differences

_THIS PROBABLY ISN'T WORTH INCLUDING_

At the moment we're not interested in the absolute counts. We would need more information about the prevalence of wifi-enabled smart phones in the population at large, as well as estimates of how many people are being double-counted in the real data. Instead, just comare the _dynamics_ by indexing each dataset to 100. 



```{r compare.plots.aggregate}

# First aggregate
cameras.avg <- aggregate(Count~ Day + Hour,      data=cameras, FUN=sum)
noggin.avg  <- aggregate(Count~ HourOfDay, data=noggin,  FUN=sum)

# Get rid of zeros (these break the scaling)
#cameras.avg[cameras.avg$Count==0,] <- 6000
#noggin.avg [noggin.avg$Count==0, ] <- 6000


# Then scale
#s <- function(data) { # index to 100
#  first <- data[1]
#  return ( (data/first) * 100 )
#}
s <- function(data) { # range 0-1
  min <- min(data)
  max <- max(data)
  return ( ( data - min) / max )
}

# For z score (not used)
z <- function(data) {
  mean <- mean(data)
  sd <- sd(data)
  return ( (data-mean) / sd )
}

noggin.avg.scaled <-  s(noggin.avg$Count)
model.avg.scaled  <-  s(cameras.avg$Count)

plot(noggin.avg.scaled, type="b")
points(model.avg.scaled, col="blue", type="b")
legend("topleft", legend=c("Real data", "Simulated data"), col=c("black","blue"), lty=1 )

```
















