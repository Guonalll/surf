---
title: "Analysing surf results across multiple runs."
author: "Nick Malleson"
date: '`r format(Sys.time(), "%d %B, %Y (%H:%M)")`'
output:
  html_document: 
    toc: yes
    pandoc_args: [
      "+RTS", "-K64m",
      "-RTS"
    ]
  pdf_document:
    fig_crop: no
    highlight: kate
    keep_tex: yes
    latex_engine: xelatex
    number_sections: yes
fontsize: 10pt
---


```{r initialise, echo=FALSE, message=FALSE, warning=FALSE}
library(GISTools)
#library(rgeos)    # For things like gIntersects
#library(rgdal)     # For reading shapefiles
#library(raster)    # For creating regular grids
#library(plyr)     # For counting and aggregating
#library(tmap)     # For thematic maps
library(classInt) # Jenks natural breaks
#library(png)      # For loading pngs after they have been written
#library(grid)     # so that they can be embedded in knitted documents
#library(spdep)    # For doing the spatial regression, contiguity matrices, etc.
#library(GWmodel)  # For geographically weighted regression (GWR)
#library(MASS)     # For stepwise regression (stepAIC())
#library(pander)   # For printing tables nicely
#library(MVN)      # For testing for multivariate normality (MVN)
library(RColorBrewer) # For making nice colour themes
#library(rgl)       # For 3D space-time cube
#library(plot3D)    # For 3D space-time cube
library(dplyr)     # To look up fields in tables (e.g. N rows higher --> lag)
library(parallel)
library(pbapply)  # For progress bar in parallel
no_cores <- detectCores() / 2  # Detect the number of cores that are available and use half (often CPUs simulate 2 threads per core)
Sys.setenv(MC_CORES=no_cores) # Run on n cores
options("mc.cores"=no_cores)
library(lubridate)
library(hexbin)   # For hex bins
library(ggplot2)  # ditto
library(gridExtra) # For arranging two grids side by side
library(feather)  # For reading data prepared by python
library(leaflet) # For doing interactive maps
```

Set up the directories. Note: The script needs the `noggin_data.feather` file to be in the working directory. This is generated by the data and scripts in:

`~/mapping/projects/frl/otley/noggin_data`

```{r get.directories }

#SCENARIO <- "ABBF-otley" # The name of the scenario to analyse
SCENARIO <- "CummutersRetired" # The name of the scenario to analyse
#SCENARIO <- "Commuters"

# The ID of the agent to analyse (graphs of all agents can be difficult to understand). If -1 then do a few at random
AGENT_ID <- -1
#AGENT_ID <- 1

# Optionally skip the first 10% of the files. We only need one day anyway, and it is better to take this day
# from towards the end of the simulation period.
# In a 14 day simulation (4032 iterations) we get 4032000 lines in the agent file (plust the header)
SKIP.ROWS <- round(4032000*0.9)

# When running this script directly in R(Studio) we need to specify the current working directory
CURRENT_DIRECTORY <- '/Users/nick/research_not_syncd/git_projects/surf/abm/surf-abm/results'

#CURRENT_DIRECTORY <- "C:/Code/surf/abm/surf-abm/results"
setwd(CURRENT_DIRECTORY) # This is only required when running directly in R(Studio)

# Projections. Useful for mapping
BNG <- CRS('+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +datum=OSGB36 +units=m +no_defs')
WGS <- CRS("+proj=longlat +ellps=WGS84 +datum=WGS84")

# Find the directories of all the model runs
scenarios <- sort(list.files(paste0("./out/",SCENARIO))) # The IDs of each run
scenarios.dirs <- vapply(X = scenarios, FUN = function(x) {paste(paste0("./out/",SCENARIO), "/", x, sep="") }, FUN.VALUE = character(1))

# If I have saved the results before, then load them. Calculating dates takes an unusually long amount of time
# After loading skip to 'choose.day' cell
#load('merge_results.RData')
```

Will analyse the `r length(scenarios)` scenarios in `r SCENARIO`

# Read and prepare the data

Read the data on the agent locations and activities as well as the camera counts and put into big tables


```{r readData1}

# TODO - include a unique id (the scenario) for each model

# Read the first result to make the tables. The add append the remaining results
agents  <- read.csv(paste0(scenarios.dirs[1],'/agents.csv'), skip = SKIP.ROWS)
#acts    <- read.csv(paste0(scenarios.dirs[1],'/agent-activities.csv'))
cameras <- read.csv(paste0(scenarios.dirs[1],'/camera-counts.csv'))

# We might have skipped rows, so get the headers
headers <- read.csv(paste0(scenarios.dirs[1],'/agents.csv'),header = F, nrows = 1, as.is = T)
colnames(agents) <- headers

# Now append the remaining results
for (i in 2:length(scenarios)) {
  s <- scenarios[i]
  dir <- scenarios.dirs[i]
  agents.temp  <- read.csv(paste0(scenarios.dirs[1],'/agents.csv'), skip = SKIP.ROWS)
  colnames(agents.temp) <- headers
  #acts.temp    <- read.csv(paste0(scenarios.dirs[1],'/agent-activities.csv'))
  cameras.temp <- read.csv(paste0(scenarios.dirs[1],'/camera-counts.csv'))
  agents  <- rbind(agents, agents.temp)
  #acts    <- rbind(acts, acts.temp)
  cameras <- rbind(cameras, cameras.temp)
#  print(paste("Read file",i))
}
rm(agents.temp, cameras.temp, s, dir) #,acts.temp)
save.image(paste0('merge_results-temp-',SCENARIO,'.RData'))
gc()
```

Now finish reading the data by creating proper date columns

```{r readData-makedates }

# Choose a day to do the analysis on so now to reduce the amount of data we need to deal with.

analysis.day <- "2011-01-14" # This is towards the end of a 2-week simulation
agents <-   agents[which(startsWith(as.character(agents$Time), analysis.day)),]
cameras <- cameras[which(cameras$Date==analysis.day),]
#acts    <-    acts[date(acts$date)   ==analysis.day,]

#stopifnot(identical(unique(date(cameras$Time)), unique(date(agents$date))))
#stopifnot(length(unique(date(cameras$date)))==1)

# Make proper time columns (sorry, this looks complicated because it's in parallel, but simply uses 'strptime' to convert the dates) . 
# The do.call is because unlist breaks lists of dates (https://stackoverflow.com/questions/15659783/why-does-unlist-kill-dates-in-r)
# Text format for agents and activities is "2011-01-01T01:10"
cl <- makeCluster(no_cores)
clusterExport(cl, "agents")
agents$date <- do.call("c", pblapply( X = 1:nrow(agents), FUN = function(i) { 
  strptime(as.character(agents[i,"Time"]), "%Y-%m-%dT%H:%M") }, cl=cl ) )
stopCluster(cl)
gc()

#acts$date    <- do.call("c", pblapply( X = 1:nrow(acts),   FUN = function(i) { strptime(as.character(  #acts[i,"Time"]), "%Y-%m-%dT%H:%M") }, cl=no_cores/2 ) )
#gc()

# Camera dates need to have date plus number of hours as these are stored in different columns (don't bother with parallel)
cameras$date <- as.POSIXct(as.Date(cameras$Date)) + ( 3600 * cameras$Hour )
# Cameras also need some extra date columns to make them easier to work with
cameras$Day <-     as.POSIXct(round(cameras$date, units="days" ))
cameras$DayNum <-  yday(cameras$date) # Number of day in yaer
#hour = as.POSIXct(round(modelDf$datetime, units="hours"))
cameras$Week <-    week(cameras$date) # (from lubridate library)
cameras$Month <-   month(cameras$date)
cameras$Weekday <- weekdays(cameras$date)

# Make the camera ID a string so that ggplot2 treats it as cetegorical
cameras$Camera <- paste0("",cameras$Camera)


# Read the real camera counts. These data were prepared by scripts and data in: ~/mapping/projects/frl/otley/noggin_data
noggin <- read_feather("noggin_data.feather")
noggin$datetime <- as.POSIXct(noggin$Timestamp)
noggin$Location <- paste0("",noggin$Location) # ID -> String
noggin$Day   <-   as.POSIXct(round(noggin$datetime, units="days" ))
noggin$DayNum <-  yday(noggin$datetime) # Number of day in year
noggin$Hour  <-   as.POSIXct(round(noggin$datetime, units="hours"))
noggin$Week  <-   floor_date(noggin$datetime, "week") # (from lubridate library)
noggin$Month <-   floor_date(noggin$datetime, "month")
noggin$Weekday <- weekdays(noggin$datetime)
noggin$HourOfDay <- as.integer(format(noggin$datetime, "%H")) # (https://stackoverflow.com/questions/10683224/obtain-hour-from-datetime-vector)
# Extract middle of the week
noggin = noggin[which( !(noggin$Location %in% c("16","19")) & noggin$Weekday %in% c("Tuesday","wednesday","Thursday") ),]
noggin[which(noggin$Location=="20"),]$Location = "17" # rename location 20 to location 17 (as per noggin metadata)
#aggnoggin = aggregate(Count~Location + HourOfDay, data=noggin, FUN=sum)
#aggnoggin$RelCount = aggnoggin$Count / sum(aggnoggin$Count)

save.image(paste0('merge_results',SCENARIO,'.RData'))

```


Choose one day from which to do the remaining analysis (_this is done in previous chunk now_)


```{r choose.day, exec=FALSE}
print(table(date(cameras$date)))
# Sanity check
#stopifnot(identical(unique(date(cameras$date)), unique(date(agents$date))) & identical(unique(date(cameras$date)), unique(date(acts$date))))
stopifnot(identical(unique(date(cameras$date)), unique(date(agents$date))))

analysis.day <- as.Date("2011-01-14")

cameras <- cameras[date(cameras$date)==analysis.day,]
#acts    <-    acts[date(acts$date)   ==analysis.day,]
agents  <-  agents[date(agents$date) ==analysis.day,]

stopifnot(identical(unique(date(cameras$date)), unique(date(agents$date))))
#stopifnot(identical(unique(date(cameras$date)), unique(date(agents$date))) & identical(unique(date(cameras$date)), unique(date(acts$date))))

stopifnot(length(unique(date(cameras$date)))==1)
```

# Plot model results 

Plot the total average hourly footfall count and 99% confidence intervals. Note that the confidence intervals are so small that they are not visible.

```{r model.hourly.total, fig.width=9, fig.height=5}
#ggplot(cameras, aes(Hour, Count, color=Camera)) + # Colouring points by camera id (doesn't work)
#ggplot(cameras[sample(1:nrow(cameras), 1000),], aes(Hour, Count)) + # Sampling N points from the cameras shows wider confidence interval (obviously)
ggplot(cameras, aes(Hour, Count)) +
  #geom_hex(bins=15) +
  #geom_point(color="black", size=0.5) + 
  #geom_smooth(method="loess", se=TRUE, level=0.99, color="green") + 
  geom_smooth(se=TRUE, level=0.99, color="black") + #GAM
  ylab("Number of agents")+
  ggtitle( "Total footfall count from all sensors over all model runs")

ggsave(paste0("model_hourly_total",SCENARIO,".pdf"),  device="pdf",  path="./merge_results-figs/", width=9, height=4)
ggsave(paste0("model_hourly_total",SCENARIO,".tiff"), path="./merge_results-figs/", width=9, height=4)

```

Plot the average hourly count per camera, again with confindence intervals.

```{r model.hourly.sensors, fig.width=9, fig.height=5 }
# Colours for the cameras 
cols <- brewer.pal(length(unique(cameras$Camera)),'Set1')
names(cols) <- unique(cameras$Camera)

plot1 <- ggplot(cameras, aes(Hour, Count, colour=Camera)) +
  #geom_point(size=1) + 
  guides(colour=guide_legend(title="Sensor")) + # Change the title of the legend
  geom_smooth(se=TRUE, level=0.99) + 
  scale_colour_manual(values=cols)+
  ylab("Number of agents")+
  ggtitle( "Total footfall count from for each sensor over all model runs")
plot1

# Uncomment below to see loess as well

#plot2 <- ggplot(cameras, aes(Hour, Count, col=Camera)) +
#  geom_point() + 
#  geom_smooth(method="loess", se=TRUE, level=0.99) +
#  ggtitle( "Footfall counts per camera over all\n model runs (smoothing with loess)")

#grid.arrange(plot1, plot2, ncol=2)

ggsave(paste0("model_hourly_sensors",SCENARIO,".pdf"),  device="pdf",  path="./merge_results-figs/", width=9, height=4)
ggsave(paste0("model_hourly_sensors",SCENARIO,".tiff"), device="tiff", path="./merge_results-figs/", width=9, height=4)

```

# Plot Real Data

Compare these to the real data

```{r plot.real.data, fig.width=9, fig.height=9 }

plot1 <- ggplot(noggin, aes(HourOfDay, Count)) +
  #geom_hex(bins=15) +
  #geom_smooth(method="loess", se=TRUE, level=0.99, color="green") + 
  geom_smooth(se=TRUE, level=0.99, color="black") + #GAM
  ylab("Number of people")+
  xlab("Hour")
  #ggtitle( "Average weekday footfall count from all sensors (real data)")

plot2 <- ggplot(noggin, aes(HourOfDay, Count, colour=Location)) +
  #geom_point(size=2) + 
  guides(colour=guide_legend(title="Sensor")) + # Change the title of the legend
  geom_smooth(se=TRUE, level=0.99)+
  scale_colour_manual(values=cols)+
  ylab("Number of people")+
  xlab("Hour")+
  ggtitle( "Average weekday footfall count per sensor (real data)")

g <- grid.arrange(plot1, plot2, nrow=2)

ggsave(paste0("plot_real_data",SCENARIO,".pdf"),  plot=g, device="pdf",  path="./merge_results-figs/", width=9, height=9)
ggsave(paste0("plot_real_data",SCENARIO,".tiff"), plot=g, device="tiff", path="./merge_results-figs/", width=9, height=9)

#ylab="Total Count"
#plot(aggregate(Count~HourOfDay, data=aggnoggin, FUN=sum), 
#     main="Footfall per hour of day (observations middle of week)", ylab=ylab, col="black", type="l"
#)
```


# Compare differences in absolute footfall counts

Make a box plot. (_Not going to use this in the paper_)

```{r compare.absolute.footdall, fig.width=11, fig.height=5}

#x <- cbind(aggregate(formula = Count ~ Hour, data = cameras, FUN=mean),
#  aggregate(formula = Count ~ HourOfDay, data = noggin, FUN=mean))


boxplot( 
  aggregate(formula = Count ~ Hour, data = cameras, FUN=mean, label="asd"),
  aggregate(formula = Count ~ HourOfDay, data = noggin, FUN=mean),
  horizontal = TRUE
)

#ggplot() +  
#  geom_boxplot(data=aggregate(formula = Count ~ Hour, data = cameras, FUN=mean), 
#               aes(Hour, Count, group="Simulated Data")) 

  #geom_boxplot(data=noggin,  aes(HourOfDay, Count, group="Real-World Data"))
  # Plot for all sensors
  #geom_boxplot(data=cameras, aes(Hour, Count, col=Camera)) + 
  #geom_boxplot(data=noggin,  aes(HourOfDay, Count, col=Location))


```


# Plot Differences Between the Model and the Real Data

On two separate plots (_this isn't being used in the paper_)

```{r compare.plots, fig.width=11, fig.height=5 }

plot1 <- ggplot(cameras, aes(Hour, Count, col=Camera)) +
  #geom_point() + 
  geom_smooth(se=TRUE, level=0.99) + 
  scale_colour_manual(values=cols)+
  ggtitle( "Total footfall counts per camera (models)")

plot2 <- ggplot(noggin, aes(HourOfDay, Count, col=Location)) +
  #geom_point() + 
  geom_smooth(se=TRUE, level=0.99) + #GAM
  scale_colour_manual(values=cols)+
  ggtitle( "Total footfall counts per camera (real data)")

grid.arrange(plot1, plot2, ncol=2)

#ylab="Total Count"
#plot(aggregate(Count~HourOfDay, data=aggnoggin, FUN=sum), 
#     main="Footfall per hour of day (observations middle of week)", ylab=ylab, col="black", type="l"
#)
```



On the same plot:

```{r compare.plots2, fig.width=11, fig.height=5}

ggplot() +
  #geom_point() + 
  geom_smooth(se=TRUE, level=0.99, data=cameras, aes(Hour, Count, col=Camera), linetype="dotted" ) + 
  geom_smooth(se=TRUE, level=0.99, data=noggin,  aes(HourOfDay, Count, col=Location)) + 
  guides(colour=guide_legend(title="Sensor")) + # Change the title of the legend
  #scale_colour_manual("Set3")+
  scale_colour_manual(values=cols)+
  ggtitle( "Comparing Model Results and 'Simulated 'Real World' Data ")

ggsave(paste0("compare_plots2",SCENARIO,".pdf"),  device="pdf",  path="./merge_results-figs/", width=9, height=5)
ggsave(paste0("compare_plots2",SCENARIO,".tiff"), device="tiff", path="./merge_results-figs/", width=9, height=5)

```

Just the average:

```{r compare.plots3, fig.width=11, fig.height=5}

ggplot() +
  #geom_point() + 
  geom_smooth(se=TRUE, level=0.99, data=cameras, aes(Hour, Count),      color="black", linetype="dotted" ) + 
  geom_smooth(se=TRUE, level=0.99, data=noggin,  aes(HourOfDay, Count), color="black") + 
  guides(colour=guide_legend(title="Sensor")) + # Change the title of the legend
  #scale_colour_manual("Set3")+
  scale_colour_manual(values=cols)+
  ggtitle( "Comparing Model Results and 'Simulated 'Real World' Data ")

ggsave(paste0("compare_plots3",SCENARIO,".pdf"),  device="pdf",  path="./merge_results-figs/", width=9, height=5)
ggsave(paste0("compare_plots3",SCENARIO,".tiff"), device="tiff", path="./merge_results-figs/", width=9, height=5)

```

# Spatial Analysis

Map the camera locations and look at what's going on. These data are stored somewhere else: `~/mapping/projects/frl/otley/noggin_data/Otley Raw to End July 2017/`

```{r read.camera.locations }

site.locations <- read.csv("~/mapping/projects/frl/otley/noggin_data/Otley Raw to End July 2017/site_locations.csv")
site.locations$LocationID <- paste0("",site.locations$LocationID) # Make the ID a string
# Remove un-used cameras
site.locations <- site.locations[which(site.locations$LocationID %in% unique(cameras$Camera)),]
stopifnot(nrow(site.locations)==length(unique(cameras$Camera)))

# Make spdf
sites <- SpatialPointsDataFrame( coords = cbind(site.locations$Lon, site.locations$Lat), 
                                 data = site.locations,
                                 proj4string = WGS)

# Make a shapefile too
#writeOGR(sites, dsn="~/mapping/projects/frl/otley/noggin_data/Otley Raw to End July 2017", layer = "site_locations", driver = "ESRI Shapefile")
```

```{r map.locations, fig.width=9, fig.height=9 }


#the.plot <- ggplot() +
#  #geom_point() + 
#  geom_smooth(se=TRUE, level=0.99, data=cameras, aes(Hour, Count, col=Camera), linetype="dotted"  ) + 
#  geom_smooth(se=TRUE, level=0.99, data=noggin,  aes(HourOfDay, Count, col=Location)) + 
#  ggtitle( "Total footfall counts per camera (models)")

popup <- paste0("<strong>ID: </strong>", sites$LocationID,
                "<br>Description:", sites$Description,
                "<br>Notes:", sites$Notes
                      )


the.map <- leaflet( data=sites ) %>%
  #addCircles(color = unname(cols), radius=10, fillOpacity = 1.0, stroke = FALSE) %>%
  #addMarkers(label=sites$LocationID, popup=popup) %>%
  addMarkers(popup=popup) %>%
  addLabelOnlyMarkers(label=sites$LocationID, labelOptions = labelOptions(noHide = T, direction = "right")) %>%
  addTiles() #%>%  # Add default OpenStreetMap map tiles

the.map
#grid.arrange(the.plot, the.map, ncol=2)

```





# Index to 100 and Calculate Differences

_THIS PROBABLY ISN'T WORTH INCLUDING_

At the moment we're not interested in the absolute counts. We would need more information about the prevalence of wifi-enabled smart phones in the population at large, as well as estimates of how many people are being double-counted in the real data. Instead, just comare the _dynamics_ by indexing each dataset to 100. 



```{r compare.plots.aggregate}

# First aggregate
cameras.avg <- aggregate(Count~ Day + Hour,      data=cameras, FUN=sum)
noggin.avg  <- aggregate(Count~ HourOfDay, data=noggin,  FUN=sum)

# Get rid of zeros (these break the scaling)
#cameras.avg[cameras.avg$Count==0,] <- 6000
#noggin.avg [noggin.avg$Count==0, ] <- 6000


# Then scale
#s <- function(data) { # index to 100
#  first <- data[1]
#  return ( (data/first) * 100 )
#}
s <- function(data) { # range 0-1
  min <- min(data)
  max <- max(data)
  return ( ( data - min) / max )
}

# For z score (not used)
z <- function(data) {
  mean <- mean(data)
  sd <- sd(data)
  return ( (data-mean) / sd )
}

noggin.avg.scaled <-  s(noggin.avg$Count)
model.avg.scaled  <-  s(cameras.avg$Count)

plot(noggin.avg.scaled, type="b")
points(model.avg.scaled, col="blue", type="b")
legend("topleft", legend=c("Real data", "Simulated data"), col=c("black","blue"), lty=1 )

```
















